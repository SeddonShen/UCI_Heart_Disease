# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12SwVptBePw2gB9qI4d6YFHYWA98c_N2T
"""

# 加载必要的数据包
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('heart.csv')
df

"""各列具体含义：
*  年龄：年龄
*  性别：（1 = 男性；0 = 女性）
*  cp：胸痛类型（1：典型心绞痛，2：非典型心绞痛，3：非心绞痛，4：无症状）
*  trestbps：静息血压（入院时以 mm Hg 为单位）
*  chol：以mg/dl计的血清胆固醇
*  fbs：（空腹血糖 > 120 mg/dl）（1：真；0：假）
*  restecg：静息心电图结果（0：正常，1：有异常，2 = 显示心室肥大）
*  thalach：达到最大心率
*  exang：运动诱发的心绞痛（1：是，0：否）
*  oldpeak：相对于休息，运动引起的 ST 压低
* slope：峰值运动ST段的斜率（1：向上倾斜，2：平坦，3：向下倾斜）
*  ca：通过荧光检查着色的主要血管 (0-3) 的数量
*  thal：地中海贫血（一种血液疾病）（3 = 正常；6 = 固定缺陷；7 = 可逆缺陷）
*  target：心脏病（0：否，1：是）

"""

# 改下名字让这些列更易于理解
df.columns = ['age', 'isMale', 'cp', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 
              'maximum_heart_rate', 'exercise_induced_angina', 'old_peak', 'slope', 'major_vessels', 
              'thal', 'heart_disease']
df

df.isnull().values.any()
# 全为false表示均不缺失值

# 展示一些数据的基本信息
df.describe()

"""**离散变量**"""

a = pd.get_dummies(df['cp'], prefix = "cp")
b = pd.get_dummies(df['thal'], prefix = "thal")
c = pd.get_dummies(df['slope'], prefix = "slope")
frames = [df, a, b, c]
df = pd.concat(frames, axis = 1)
df.columns
df = df.drop(columns = ['cp', 'thal', 'thal_0', 'slope'])
# renaming categorical columns
df = df.rename(columns={
  'cp_0':'typical_angina_pain',
  'cp_1':'atypical_angina_pain',
  'cp_2':'non-anginal_pain',
  'cp_3':'asymptomatic_pain',
  'slope_0':'upsloping_st', 'slope_1':'flat_st', 'slope_2':'downsloping_st',
  'thal_1': 'thalassemia_normal', 'thal_2': 'thalassemia_fixed', 'thal_3': 'thalassemia_reversable'})
df

plt.figure(figsize=(15,15))
matrix = df.corr()
mask1 = np.triu(np.ones_like(matrix, dtype=bool))
mask2 = np.abs(matrix) <= 0.5
mask  = mask1 | mask2
sns.heatmap(df.corr(),annot=True,fmt='.2f')
plt.show()

plt.figure(figsize=(11, 9),dpi=400)
sns.heatmap(
    df.corr(),            
    vmax=0.3,             
    #  cmap=palettable.cmocean.diverging.Curl_10.mpl_colors,            
    annot=True,            
    fmt=".2f",            
    annot_kws={'size':8,'weight':'normal', 'color':'#253D24'},            
    mask=np.triu(np.ones_like(df.corr(),dtype=np.bool)),#显示对脚线下面部分图            
    square=True, linewidths=.5,#每个方格外框显示，外框宽度设置           
    cbar_kws={"shrink": .5}           )
plt.show()

# ignoring divide error and pair plotting
with np.errstate(divide='ignore',invalid='ignore'):
    sns.pairplot(df, hue="heart_disease", palette="husl")
plt.show()

plt.figure(figsize=(15,8))
plt.title('Age Distribution')
plt.ylabel('Frequency')
plt.xlabel('Age')
sns.distplot(df['age'],color='#E8710A',bins=20)

pd.crosstab(df.age,df.heart_disease).plot(kind="bar",figsize=(15,8), color=['#282C34','#E8710A' ])
plt.title('Heart Disease and Age Relationship')
plt.legend(["Doesn't have", "Heart disease"])
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

pd.crosstab(df.isMale,df.heart_disease).plot(kind="bar",figsize=(15,8), color=['#282C34','#E8710A' ])
plt.title('Gender')
plt.xlabel('Gender (0 : Female, 1 : Male)')
plt.xticks(rotation=0)
plt.legend(["Doesn't have", "Have heart disease"])
plt.ylabel('Frequency(Number of people)')
plt.show()

x_data = df.drop(['heart_disease'], axis=1)

# normalization
X = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values
y = df['heart_disease']

# 对x进行处理和规范化

# 模型的选择
# SVM：支持向量机算法的目标是在空间中找到一个对数据点进行明确分类的超平面。
# 为了分离两类数据点，可以选择许多可能的超平面。 
# 我们的目标是找到一个具有最大边距的平面，即两个类的数据点之间的最大距离。 
# 最大化边缘距离提供了一些强化，以便可以更有信心地对未来的数据点进行分类。
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
# 分离测试集和训练集

from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, recall_score

svc=SVC(kernel= 'rbf', gamma = 'scale' ,random_state=42)
svc.fit(X_train, y_train)
svm_pred = svc.predict(X_test)

# storing recall_score for later comparision
svm_recall = round(recall_score(y_test,svm_pred,average='weighted'),3)

print (classification_report(y_test, svm_pred, labels=None, target_names=None, sample_weight=None, digits=3, output_dict=False))
acc = accuracy_score(y_test, svm_pred)
print ("Accuracy: %.3f" % acc)

# 使用混淆矩阵 分析分类效果
from sklearn.metrics import plot_confusion_matrix
disp = plot_confusion_matrix(svc, X_test, y_test,
                             cmap=plt.cm.inferno)
disp.ax_.set_title("Confusion Matrix")
plt.show()

from matplotlib import cm
from sklearn.metrics import roc_curve, auc

plt.figure(figsize = (15,8))
plt.xlim([-0.01, 1.00])
plt.ylim([-0.01, 1.01])
for g in [0.01, 0.1, 0.20, 1, 'scale', 'auto']:
    svm = SVC(gamma=g).fit(X_train, y_train)
    y_score_svm = svm.decision_function(X_test)
    fpr_svm, tpr_svm, _ = roc_curve(y_test, y_score_svm)
    roc_auc_svm = auc(fpr_svm, tpr_svm)
    accuracy_svm = svm.score(X_test, y_test)
    print("gamma = {}  accuracy = {:.2f}   AUC = {:.3f}".format(g, accuracy_svm, 
                                                                    roc_auc_svm))
    plt.plot(fpr_svm, tpr_svm, lw=3, alpha=0.7, 
             label='SVM (gamma = {}, auc = {:0.3f})'.format(g, roc_auc_svm))

plt.xlabel('False Positive rate', fontsize=16)
plt.ylabel('True Positive rate (Recall)', fontsize=16)
plt.plot([0, 1], [0, 1], color='k', lw=0.5, linestyle='--')
plt.legend(loc="lower right", fontsize=11)
plt.title('ROC Curve: (1-of-1)', fontsize=20)
plt.axes().set_aspect('equal')

plt.show()

"""**K近邻（KNN）方法**

KNN 算法假设相似的事物存在于附近。 换句话说，相似的事物彼此接近。 KNN 通过计算图上点之间的距离来捕捉相似性的概念。
"""

def plot_roc_curve(fprs,tprs):
    plt.figure(figsize=(8,6),dpi=80)
    plt.plot(fprs,tprs)
    plt.plot([0,1],linestyle='--')
    plt.xticks(fontsize=13)
    plt.yticks(fontsize=13)
    plt.ylabel('TP rate',fontsize=15)
    plt.xlabel('FP rate',fontsize=15)
    plt.title('ROC',fontsize=17)
    plt.show()

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=8,metric='minkowski')
knn.fit(X_train, y_train)
knn_pred = knn.predict(X_test)
# storing recall_score for later comparision
knn_recall = round(recall_score(y_test,knn_pred,average='weighted'),3)
print (classification_report(y_test, knn_pred, labels=None, target_names=None, sample_weight=None, digits=3, output_dict=False))
print ("Accuracy: %.3f" % accuracy_score(y_test, knn_pred))
from sklearn.metrics import plot_confusion_matrix
disp = plot_confusion_matrix(knn, X_test, y_test,cmap=plt.cm.inferno)
disp.ax_.set_title("Confusion Matrix")
plt.show()
knn_clf = KNeighborsClassifier()
y_probabilities = knn.predict_proba(X_test)[:,1]
from sklearn.metrics import roc_curve
fprs2,tprs2,thresholds2 = roc_curve(y_test,y_probabilities)
# 此处调用前面的绘制函数
plot_roc_curve(fprs2,tprs2)

"""**Decision** **Trees**


A Decision tree is a flowchart like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label.
"""

from sklearn import tree

tree = tree.DecisionTreeClassifier(random_state=42)
tree.fit(X_train, y_train)

# predict the target on the test dataset
tree_predict_test = tree.predict(X_test)

tree_recall = round(recall_score(y_test, tree_predict_test, average='weighted'),3)

# Feature Importances

# Calculate feature importances
importances = tree.feature_importances_
# Sort feature importances in descending order
indices = np.argsort(importances)[::-1]

# Rearrange feature names so they match the sorted feature importances
names = [X.columns[i] for i in indices]

plt.figure(figsize = (22,12))
# Barplot: Add bars
plt.bar(range(X.shape[1]), importances[indices])
# Add feature names as x-axis labels
plt.xticks(range(X.shape[1]), names, rotation=90)
# Create plot title
plt.title("Feature Importances")
plt.ylabel('Importances')
# Show plot
plt.show()
print (classification_report(y_test, tree_predict_test, labels=None, target_names=None, sample_weight=None, digits=3, output_dict=False))

# 混淆矩阵
disp = plot_confusion_matrix(tree, X_test, y_test,cmap=plt.cm.inferno)
disp.ax_.set_title("Confusion Matrix")
plt.show()

y_probabilities = tree.predict_proba(X_test)[:,1]
from sklearn.metrics import roc_curve
fprs3,tprs3,thresholds3 = roc_curve(y_test,y_probabilities)
# 此处调用前面的绘制函数
plot_roc_curve(fprs3,tprs3)

"""方法对比"""

recall_dict = {'SVM':svm_recall,
               'KNN': knn_recall,
               'Decision Trees': tree_recall,
               }
pr_df = pd.DataFrame(recall_dict,index=['Recall Score'])
d = dict(selector="th", props=[('text-align', 'center')])
pr_df.style.set_properties(**{'width':'10em', 'text-align':'center'}).set_table_styles([d]) 
pr_df

y_pos = np.arange(len(recall_dict))
plt.bar(y_pos, recall_dict.values(), align='center', alpha=0.5)
plt.xticks(y_pos, recall_dict.keys())
plt.ylabel('Recall')
plt.title('Models Comparision')
plt.show()